{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caltech Machine Learning Homework # 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from itertools import product\n",
    "import scipy.special\n",
    "from scipy import optimize\n",
    "import scipy.optimize as spo\n",
    "from sympy import Symbol, Derivative\n",
    "import functools\n",
    "from sklearn import svm, model_selection\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, RepeatedKFold\n",
    "\n",
    "def dbg():\n",
    "    import pdb; pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: https://work.caltech.edu/homework/hw8.pdf\n",
    "\n",
    "Answers: http://work.caltech.edu/homework/hw8_sol.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primal vs Dual Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/primalvsdual.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original formulation was"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/primalvsdual2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which looks like **[b]** to me because of $\\textbf x_{n}$ terms and $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But thats' wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Wikipedia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/quadprog.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the variables are really the $\\textbf w$ terms here, subject to linear contraints involving $\\textbf x_{n}$. Hence it's a quadratic programming problem with $d+1$ variables ($+1$ since the term $\\textbf b$ appears in the constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the original formulation depends on the input space size $d$, whereas the dual formulation only depends on $N$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Soft Margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/softmargsvm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.loadtxt('data/hw8/features.train.txt')\n",
    "test = np.loadtxt('data/hw8/features.test.txt')\n",
    "\n",
    "X_train = train[:,1:]\n",
    "Y_train = train[:,0]\n",
    "N_train = X_train[:, 0].size\n",
    "\n",
    "X_test = test[:,1:]\n",
    "Y_test = test[:,0]\n",
    "N_test = X_test[:, 0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/polykern1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of 0 versus all was 0.8883906327852517\n",
      "Score of 2 versus all was 0.9013452914798207\n",
      "Score of 4 versus all was 0.9003487792725461\n",
      "Score of 6 versus all was 0.9152964623816642\n",
      "Score of 8 versus all was 0.9172894867962132\n"
     ]
    }
   ],
   "source": [
    "C = .01\n",
    "Q = 2\n",
    "\n",
    "def make_binary(Ys, classToKeep):\n",
    "    return np.array([1 if y == classToKeep else -1 for y in Ys])\n",
    "\n",
    "def score(y_class):\n",
    "    # Set all other labels to 0 to make it a binary y_class vs all classification\n",
    "    bin_Y_train = make_binary(Y_train, y_class)\n",
    "    bin_Y_test = make_binary(Y_test, y_class)\n",
    "    \n",
    "    clf = svm.SVC(kernel='poly', C=C, degree=Q, gamma=1.0, coef0=1.0, cache_size=20000)\n",
    "    clf.fit(X_train, bin_Y_train)\n",
    "    print(f\"Score of {y_class} versus all was {clf.score(X_test, bin_Y_test)}\")\n",
    "    \n",
    "score(0)\n",
    "score(2)\n",
    "score(4)\n",
    "score(6)\n",
    "score(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the highest $E_{in}$ was at **[a]** 0 versus all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/polykern2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of 1 versus all was 0.9780767314399601\n",
      "Score of 3 versus all was 0.9172894867962132\n",
      "Score of 5 versus all was 0.9202790234180369\n",
      "Score of 7 versus all was 0.9267563527653214\n",
      "Score of 9 versus all was 0.9118086696562033\n"
     ]
    }
   ],
   "source": [
    "score(1)\n",
    "score(3)\n",
    "score(5)\n",
    "score(7)\n",
    "score(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the lowest $E_{in}$ was at **[a]** 1 versus all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0s are hardest to detect, 1s easiest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/polykern3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1793"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_num_svs(y_class):\n",
    "    # Set all other labels to 0 to make it a binary y_class vs all classification\n",
    "    bin_Y_train = make_binary(Y_train, y_class)\n",
    "    bin_Y_test = make_binary(Y_test, y_class)\n",
    "    \n",
    "    clf = svm.SVC(kernel='poly', C=C, degree=Q, gamma=1.0, coef0=1.0, cache_size=20000)\n",
    "    clf.fit(X_train, bin_Y_train)\n",
    "    return sum(clf.n_support_)\n",
    "\n",
    "get_num_svs(0) - get_num_svs(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's closest by **[c]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/polykern4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# support vectors for 1 vs 5 with C=0.001 was 76\n",
      "E_in for 1 vs 5 with C=0.001 was 0.9834905660377359\n",
      "\n",
      "# support vectors for 1 vs 5 with C=0.01 was 34\n",
      "E_in for 1 vs 5 with C=0.01 was 0.9811320754716981\n",
      "\n",
      "# support vectors for 1 vs 5 with C=0.1 was 24\n",
      "E_in for 1 vs 5 with C=0.1 was 0.9811320754716981\n",
      "\n",
      "# support vectors for 1 vs 5 with C=1 was 24\n",
      "E_in for 1 vs 5 with C=1 was 0.9811320754716981\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = 2\n",
    "Cs = [.001, .01, .1, 1]\n",
    "\n",
    "def get_1v1_data(data, fst, snd):\n",
    "    X,Y = data\n",
    "    idxs = (Y == fst) | (Y == snd)\n",
    "    return (X[idxs], Y[idxs])\n",
    "    \n",
    "\n",
    "def score_1v5(C):\n",
    "    # Keep only data for 1s and 5s\n",
    "    X_test1v5,Y_test1v5 = get_1v1_data((X_test, Y_test), 1, 5)\n",
    "    X_train1v5, Y_train1v5 = get_1v1_data((X_train, Y_train), 1, 5)\n",
    "    \n",
    "    clf = svm.SVC(kernel='poly', C=C, degree=Q, gamma=1.0, coef0=1.0, cache_size=20000)\n",
    "    clf.fit(X_train1v5, Y_train1v5)\n",
    "    print(f\"# support vectors for 1 vs 5 with C={C} was {sum(clf.n_support_)}\")\n",
    "    print(f\"E_in for 1 vs 5 with C={C} was {clf.score(X_test1v5, Y_test1v5)}\")\n",
    "    print()\n",
    "\n",
    "[score_1v5(C) for C in Cs]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[a]** is almost true but not strictly so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[b]** is false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[c]** we don't know since we only have a bound on $E_{out}$, plus the bound is not going down strictly anyways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[d]** is a bit hard to tell because the $E_{in}$ values are equal for $C={0.01, 0.1, 1}$ but I would say yes this is true,\n",
    "\n",
    "it also makes intuitive sense because higher C means less regularization,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[d]** is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/polykern5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# support vectors with C=0.0001 at Q=2 was 236\n",
      "E_in with C=0.0001 at Q=2 was 0.9834905660377359\n",
      "\n",
      "# support vectors with C=0.0001 at Q=5 was 26\n",
      "E_in with C=0.0001 at Q=5 was 0.9811320754716981\n",
      "\n",
      "# support vectors with C=0.001 at Q=2 was 76\n",
      "E_in with C=0.001 at Q=2 was 0.9834905660377359\n",
      "\n",
      "# support vectors with C=0.001 at Q=5 was 25\n",
      "E_in with C=0.001 at Q=5 was 0.9787735849056604\n",
      "\n",
      "# support vectors with C=0.01 at Q=2 was 34\n",
      "E_in with C=0.01 at Q=2 was 0.9811320754716981\n",
      "\n",
      "# support vectors with C=0.01 at Q=5 was 23\n",
      "E_in with C=0.01 at Q=5 was 0.9787735849056604\n",
      "\n",
      "# support vectors with C=1 at Q=2 was 24\n",
      "E_in with C=1 at Q=2 was 0.9811320754716981\n",
      "\n",
      "# support vectors with C=1 at Q=5 was 21\n",
      "E_in with C=1 at Q=5 was 0.9787735849056604\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs = [2, 5]\n",
    "Cs = [.0001, .001, .01, 1]\n",
    "    \n",
    "\n",
    "def score_1v5(C, Q):\n",
    "    # Keep only data for 1s and 5s\n",
    "    X_test1v5,Y_test1v5 = get_1v1_data((X_test, Y_test), 1, 5)\n",
    "    X_train1v5, Y_train1v5 = get_1v1_data((X_train, Y_train), 1, 5)\n",
    "    \n",
    "    clf = svm.SVC(kernel='poly', C=C, degree=Q, gamma=1.0, coef0=1.0, cache_size=20000)\n",
    "    clf.fit(X_train1v5, Y_train1v5)\n",
    "    print(f\"# support vectors with C={C} at Q={Q} was {sum(clf.n_support_)}\")\n",
    "    print(f\"E_in with C={C} at Q={Q} was {clf.score(X_test1v5, Y_test1v5)}\")\n",
    "    print()\n",
    "\n",
    "[score_1v5(C, Q) for C in Cs for Q in Qs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[a]**: $E_{in}$ is lower at $Q=5$, which makes sense, so False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[b]**: Yes, the number of support vectors is actually lower here at $Q=5$. I have no idea why that is, but **[b]** seems correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/xvalidation1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_CV for [a] is 0.9897517556753229.\n",
      "E_CV for [b] is 0.994871794871795.\n",
      "E_CV for [c] is 0.99551282051282.\n",
      "E_CV for [d] is 0.994871794871795.\n",
      "E_CV for [e] is 0.99551282051282.\n"
     ]
    }
   ],
   "source": [
    "Q = 2\n",
    "RUNS = 100\n",
    "\n",
    "X_test1v5,Y_test1v5 = get_1v1_data((X_test, Y_test), 1, 5)\n",
    "X_train1v5, Y_train1v5 = get_1v1_data((X_train, Y_train), 1, 5)\n",
    "\n",
    "Cs = [.0001, .001, .01, .1, 1]\n",
    "\n",
    "def x_validate(C):\n",
    "    clf = svm.SVC(kernel='poly', C=C, degree=Q, gamma=1.0, coef0=1.0, cache_size=20000)\n",
    "    scores = model_selection.cross_val_score(clf, X_train1v5, Y_train1v5, cv=10)\n",
    "    return scores.mean()\n",
    "\n",
    "# We're being lazy and calculating the expected value of E_CV intead of selecting after each run, let's see if that works\n",
    "E_cv_a = np.array([x_validate(Cs[0]) for _ in range(RUNS)]).mean()\n",
    "print(f\"E_CV for [a] is {E_cv_a}.\")\n",
    "E_cv_b = np.array([x_validate(Cs[1]) for _ in range(RUNS)]).mean()\n",
    "print(f\"E_CV for [b] is {E_cv_b}.\")\n",
    "E_cv_c = np.array([x_validate(Cs[2]) for _ in range(RUNS)]).mean()\n",
    "print(f\"E_CV for [c] is {E_cv_c}.\")\n",
    "E_cv_d = np.array([x_validate(Cs[3]) for _ in range(RUNS)]).mean()\n",
    "print(f\"E_CV for [d] is {E_cv_d}.\")\n",
    "E_cv_e = np.array([x_validate(Cs[4]) for _ in range(RUNS)]).mean()\n",
    "print(f\"E_CV for [e] is {E_cv_e}.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates **[a]** as the clear winner, if taking expected value is indeed okay here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like that's not okay though. Let's do it properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cs:  [1.e-04 1.e-03 1.e-02 1.e-01 1.e+00]\n",
      "Counts:  [377 512  41  27  43]\n"
     ]
    }
   ],
   "source": [
    "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=100)\n",
    "winners = []\n",
    "for train_idx, val_idx in rskf.split(X_train1v5, Y_train1v5):\n",
    "    X_train, X_val = X_train1v5[train_idx], X_train1v5[val_idx]\n",
    "    Y_train, Y_val = Y_train1v5[train_idx], Y_train1v5[val_idx]\n",
    "    \n",
    "    bestScore = 0\n",
    "    winner = None\n",
    "    for C in Cs:\n",
    "        clf = svm.SVC(kernel='poly', C=C, degree=Q, gamma=1.0, coef0=1.0, cache_size=20000)\n",
    "        clf.fit(X_train, Y_train)\n",
    "        score = clf.score(X_val, Y_val)\n",
    "        if score > bestScore:\n",
    "            bestScore = score\n",
    "            winner = C\n",
    "            \n",
    "    winners.append(winner)\n",
    "\n",
    "Cs, counts = np.unique(winners, return_counts=True)\n",
    "print(\"Cs: \", Cs)\n",
    "print(\"Counts: \", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So **[b]** is the winner under the correct heuristic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/xvalidation2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004766536011758915"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = 1e-3\n",
    "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=100)\n",
    "clf = svm.SVC(kernel='poly', C=C, degree=Q, gamma=1.0, coef0=1.0, cache_size=20000)\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, X_train1v5, Y_train1v5, cv=rskf)\n",
    "1 - scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's closest to **[c]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/rbf1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.004270462633451988, 0.004982206405693912, 0.003558718861209953, 0.002846975088967918, 0.000711743772242035]\n"
     ]
    }
   ],
   "source": [
    "Cs = [0.01, 1, 100, 1e4, 1e6]\n",
    "\n",
    "X_test1v5,Y_test1v5 = get_1v1_data((X_test, Y_test), 1, 5)\n",
    "X_train1v5, Y_train1v5 = get_1v1_data((X_train, Y_train), 1, 5)\n",
    "\n",
    "def compute_eIN(C):\n",
    "    clf = svm.SVC(kernel='rbf', C=C, degree=Q, gamma=1.0, cache_size=20000)\n",
    "    clf.fit(X_train1v5, Y_train1v5)\n",
    "    return 1 - clf.score(X_train1v5, Y_train1v5)\n",
    "    \n",
    "print([compute_eIN(C) for C in Cs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That would be **[e]**, or the lowest regularization (least soft version). Which makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/rbf2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02358490566037741, 0.021226415094339646, 0.018867924528301883, 0.02358490566037741, 0.02358490566037741]\n"
     ]
    }
   ],
   "source": [
    "Cs = [0.01, 1, 100, 1e4, 1e6]\n",
    "\n",
    "X_test1v5,Y_test1v5 = get_1v1_data((X_test, Y_test), 1, 5)\n",
    "X_train1v5, Y_train1v5 = get_1v1_data((X_train, Y_train), 1, 5)\n",
    "\n",
    "def compute_eOUT(C):\n",
    "    clf = svm.SVC(kernel='rbf', C=C, degree=Q, gamma=1.0, cache_size=20000)\n",
    "    clf.fit(X_train1v5, Y_train1v5)\n",
    "    return 1 - clf.score(X_test1v5, Y_test1v5)\n",
    "    \n",
    "print([compute_eOUT(C) for C in Cs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is **[c]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 - python",
   "language": "python",
   "name": "ipython_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
